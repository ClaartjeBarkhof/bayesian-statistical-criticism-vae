We propose a framework for the statistical evaluation of variational auto-encoders (VAEs) and test two instances of this framework in the context of modelling images of handwritten digits and a corpus of English text.
Our take on evaluation is based on the idea of statistical model criticism, popular in Bayesian data analysis, whereby a statistical model is evaluated in terms of its ability to reproduce statistics of an unknown data generating process from which we can obtain samples.
A VAE learns not one, but two joint distributions over a shared sample space,
each exploiting a choice of factorisation that makes sampling tractable in one of two directions (latent-to-data, data-to-latent).  
We evaluate samples from these distributions, assessing their (marginal) fit to the observed data and our choice of prior, and we also evaluate samples through a pipeline that connects the two distributions starting from a data sample, assessing whether together they exploit and reveal latent factors of variation that are useful to a practitioner. 
We show that this methodology offers possibilities for model selection qualitatively beyond intrinsic evaluation metrics and at a finer granularity than commonly used statistics can offer.


%\wanote{Something on the nature of our framework, something on how it overcomes limitations of other forms of criticism being complementary to those, something on its use of data analysis models and how those give the analyst a knob to control the focus of the analysis, allowing us to align model criticism with the interests of the practitioner.}
%In both cases, our evaluation determines a degree of compatibility between a fixed and possibly unknown process (\eg, prior or a dataset of observations) and model samples obtained with or without access to a conditioning data sample. 

% We can note that our method gives a different ordering and can distinguish models at a finer level of granularity than the intrinsic metrics can give us along axes we practically care about.