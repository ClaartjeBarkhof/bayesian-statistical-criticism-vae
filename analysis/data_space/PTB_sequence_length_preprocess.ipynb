{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_DIR = \"/home/cbarkhof/fall-2021\"\n",
    "ANALYSIS_DIR = f\"{CODE_DIR}/analysis/analysis-files\"\n",
    "CHECKPOINT_DIR = f\"{CODE_DIR}/run_files/checkpoints\"\n",
    "\n",
    "N_UNCONDITIONAL_SAMPLES = 2000\n",
    "N_VALID_CONDITIONAL_SAMPELS = 2000\n",
    "N_TRAIN_SAMPLES = 2000\n",
    "\n",
    "COND_SAMPLE_BATCH_SIZE = 200\n",
    "\n",
    "SAMPLE_FILE = f\"generative-samples.pt\"\n",
    "CONDITIONAL_SAMPLE_FILE = f\"generative-conditional-samples.pt\"\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "import sys; sys.path.append(CODE_DIR)\n",
    "\n",
    "import torch\n",
    "\n",
    "from analysis.language_analysis.analysis_steps import make_run_overview_df, get_test_validation_loader\n",
    "from analysis.MNIST_analysis.analysis_utils import get_n_data_samples_x_y\n",
    "from utils import load_checkpoint_model_for_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_file None\n",
      "objective VAE\n",
      "beta_beta 1.0\n",
      "free_bits 5.0\n",
      "free_bits_per_dimension True\n",
      "mdr_value 16.0\n",
      "mdr_constraint_optim_lr 0.001\n",
      "info_lambda_1_rate 1.0\n",
      "info_lambda_2_mmd 100.0\n",
      "rate_constraint_val 16.0\n",
      "rate_constraint_rel eq\n",
      "rate_constraint_lr 0.001\n",
      "mmd_constraint_val 0.005\n",
      "mmd_constraint_rel le\n",
      "mmd_constraint_lr 0.001\n",
      "batch_size 32\n",
      "max_steps 1000000\n",
      "max_epochs 120\n",
      "eval_ll_every_n_epochs 1\n",
      "gen_opt adam\n",
      "gen_lr 1e-05\n",
      "gen_l2_weight 0.0001\n",
      "gen_momentum 0.0\n",
      "inf_opt adam\n",
      "inf_lr 1e-05\n",
      "inf_l2_weight 0.0001\n",
      "inf_momentum 0.0\n",
      "max_gradient_norm 1.0\n",
      "iw_n_samples 50\n",
      "latent_dim 32\n",
      "decoder_network_type strong_distil_roberta_decoder\n",
      "decoder_MADE_gating True\n",
      "decoder_MADE_gating_mechanism 0\n",
      "strong_roberta_decoder_embedding_dropout False\n",
      "strong_roberta_decoder_embedding_dropout_prob 0.2\n",
      "decoder_MADE_hidden_sizes 200-220\n",
      "encoder_network_type distil_roberta_encoder\n",
      "encoder_MADE_hidden_sizes 200-220\n",
      "q_z_x_type independent_gaussian\n",
      "p_z_type isotropic_gaussian\n",
      "mog_n_components 10\n",
      "data_dir /home/cbarkhof/fall-2021/data\n",
      "image_or_language language\n",
      "data_distribution categorical\n",
      "image_dataset_name bmnist\n",
      "image_w_h 28\n",
      "n_channels 1\n",
      "tokenizer_name roberta-base\n",
      "language_dataset_name ptb\n",
      "vocab_size 50265\n",
      "num_workers 2\n",
      "pin_memory True\n",
      "max_seq_len 64\n",
      "print_stats True\n",
      "print_every_n_steps 1\n",
      "logging False\n",
      "checkpointing False\n",
      "log_every_n_steps 1\n",
      "wandb_project fall-2021-VAE\n",
      "gpus 1\n",
      "ddp False\n",
      "code_dir /home/cbarkhof/fall-2021\n",
      "job_id \n",
      "short_dev_run False\n",
      "run_name_prefix \n",
      "run_name VAE | STRONG | Nz=32\n",
      "checkpoint_dir /home/cbarkhof/fall-2021/run_files/checkpoints/\n",
      "wandb_dir /home/cbarkhof/fall-2021/run_files/\n"
     ]
    }
   ],
   "source": [
    "from dataset_dataloader import LanguageDataset\n",
    "from arguments import prepare_parser\n",
    "\n",
    "config = prepare_parser(jupyter=True, print_settings=False)\n",
    "for k, v in vars(config).items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New pre-processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1215 13:04:37.576871 23010757607168 builder.py:539] Reusing dataset yahoo_answers_topics (/home/cbarkhof/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/b2712a72fde278f1d6e96cc4f485fd89ed2f79ecb231441e13645b53da021902)\n",
      "W1215 13:04:38.559062 23010757607168 builder.py:539] Reusing dataset yahoo_answers_topics (/home/cbarkhof/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/b2712a72fde278f1d6e96cc4f485fd89ed2f79ecb231441e13645b53da021902)\n",
      "W1215 13:04:39.019777 23010757607168 builder.py:539] Reusing dataset yahoo_answers_topics (/home/cbarkhof/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/b2712a72fde278f1d6e96cc4f485fd89ed2f79ecb231441e13645b53da021902)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945435a7de1a43cd86c052f1e4f1100e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "--> data_path /home/cbarkhof/fall-2021/data/yahoo_answer-roberta-base-seqlen-500\n",
      "Saved split train in /home/cbarkhof/fall-2021/data/yahoo_answer-roberta-base-seqlen-500/train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbec8259b10d4ceca2f546f6609f21ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "--> data_path /home/cbarkhof/fall-2021/data/yahoo_answer-roberta-base-seqlen-500\n",
      "Saved split validation in /home/cbarkhof/fall-2021/data/yahoo_answer-roberta-base-seqlen-500/validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba133c224c349d98c7d078ef001d080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "convert to features\n",
      "--> data_path /home/cbarkhof/fall-2021/data/yahoo_answer-roberta-base-seqlen-500\n",
      "Saved split test in /home/cbarkhof/fall-2021/data/yahoo_answer-roberta-base-seqlen-500/test\n"
     ]
    }
   ],
   "source": [
    "config.max_seq_len = 500\n",
    "config.batch_size = 10\n",
    "config.language_dataset_name = \"yahoo_answer\"\n",
    "\n",
    "dataset = LanguageDataset(args=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/20\r"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for i, batch in enumerate(dataset.train_loader(batch_size=20)):\n",
    "    print(f\"{i:2d}/20\", end=\"\\r\")\n",
    "    lengths.append(batch[\"attention_mask\"].sum(dim=-1)-2)\n",
    "    if i == 100:\n",
    "        break\n",
    "lengths = torch.cat(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1069.,  434.,  216.,   88.,   69.,   42.,   30.,   19.,    8.,\n",
       "          45.]),\n",
       " array([  0. ,  49.8,  99.6, 149.4, 199.2, 249. , 298.8, 348.6, 398.4,\n",
       "        448.2, 498. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(lengths.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is file!\n",
      "train 140000\n",
      "validation 6000\n",
      "test 6000\n",
      "Returns concatted input_ids, attention_masks tensors\n",
      "Is file!\n",
      "train 140000\n",
      "validation 6000\n",
      "test 6000\n",
      "Returns concatted input_ids, attention_masks tensors\n",
      "Is file!\n",
      "train 42068\n",
      "validation 3370\n",
      "test 3761\n",
      "Returns concatted input_ids, attention_masks tensors\n",
      "Is file!\n",
      "train 42068\n",
      "validation 3370\n",
      "test 3761\n",
      "Returns concatted input_ids, attention_masks tensors\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# YAHOO\n",
    "yahoo_train_inputs, yahoo_train_atts = get_n_data_samples_x_y(\n",
    "    language_dataset_name=\"yahoo_answer\",\n",
    "    image_or_language=\"language\", \n",
    "    shuffle=False,\n",
    "    N_samples=N_TRAIN_SAMPLES, \n",
    "    phase=\"train\")\n",
    "\n",
    "yahoo_valid_inputs, yahoo_valid_atts = get_n_data_samples_x_y(\n",
    "    language_dataset_name=\"yahoo_answer\",\n",
    "    image_or_language=\"language\", \n",
    "    shuffle=False,\n",
    "    N_samples=N_VALID_CONDITIONAL_SAMPELS, \n",
    "    phase=\"valid\")\n",
    "\n",
    "yahoo_valid_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(yahoo_valid_inputs, yahoo_valid_atts), \n",
    "                                batch_size=COND_SAMPLE_BATCH_SIZE, pin_memory=True)\n",
    "\n",
    "# ---------------------------------\n",
    "# PTB\n",
    "ptb_train_inputs, ptb_train_atts = get_n_data_samples_x_y(\n",
    "    language_dataset_name=\"ptb\",\n",
    "    image_or_language=\"language\", \n",
    "    shuffle=False,\n",
    "    N_samples=N_TRAIN_SAMPLES, \n",
    "    phase=\"train\")\n",
    "\n",
    "ptb_valid_inputs, ptb_valid_atts = get_n_data_samples_x_y(\n",
    "    language_dataset_name=\"ptb\",\n",
    "    image_or_language=\"language\", \n",
    "    shuffle=False,\n",
    "    N_samples=N_VALID_CONDITIONAL_SAMPELS, \n",
    "    phase=\"valid\")\n",
    "\n",
    "ptb_valid_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(ptb_valid_inputs, ptb_valid_atts), \n",
    "                                batch_size=COND_SAMPLE_BATCH_SIZE, pin_memory=True)\n",
    "\n",
    "# ---------------------------------\n",
    "# ALL TOGETHER\n",
    "all_loaders = dict(yahoo=yahoo_valid_loader, ptb=ptb_valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_of_chunk(chunk, eos_token_id=2, bos_token=0, remove_start=False):\n",
    "    lens = []\n",
    "    for s_i in chunk.tolist():\n",
    "        # if it is a weak decoder, the start token needs to be cut-off as well (if predicted)\n",
    "        if s_i[0] == bos_token and remove_start:\n",
    "            s_i = s_i[1:]\n",
    "        length = s_i.index(eos_token_id) if eos_token_id in s_i else len(s_i)\n",
    "        lens.append(length)\n",
    "    return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb_valid_lens = get_length_of_chunk(ptb_valid_inputs, remove_start=True)\n",
    "ptb_train_lens = get_length_of_chunk(ptb_train_inputs, remove_start=True)\n",
    "yahoo_valid_lens = get_length_of_chunk(yahoo_valid_inputs, remove_start=True)\n",
    "yahoo_train_lens = get_length_of_chunk(yahoo_train_inputs, remove_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 62\n",
      "1 62\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 62\n",
      "1 62\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n",
      "2000 2000\n",
      "2000 2000\n",
      "2000 2000\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(min(ptb_valid_lens), max(ptb_valid_lens))\n",
    "print(min(ptb_train_lens), max(ptb_train_lens))\n",
    "plt.hist(ptb_valid_lens, bins=62)\n",
    "plt.hist(ptb_train_lens, bins=62)\n",
    "plt.show()\n",
    "\n",
    "print(min(yahoo_valid_lens), max(yahoo_valid_lens))\n",
    "print(min(yahoo_train_lens), max(yahoo_train_lens))\n",
    "plt.hist(yahoo_valid_lens, bins=62)\n",
    "plt.hist(yahoo_train_lens, bins=62)\n",
    "plt.show()\n",
    "\n",
    "print(len(ptb_valid_lens), len(ptb_valid_inputs))\n",
    "print(len(ptb_train_lens), len(ptb_train_inputs))\n",
    "print(len(yahoo_valid_lens), len(yahoo_valid_inputs))\n",
    "print(len(yahoo_train_lens), len(yahoo_train_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objective</th>\n",
       "      <th>dataset</th>\n",
       "      <th>l_rate</th>\n",
       "      <th>dropout</th>\n",
       "      <th>beta_beta</th>\n",
       "      <th>free_bits</th>\n",
       "      <th>mdr_value</th>\n",
       "      <th>l_mmd</th>\n",
       "      <th>decoder</th>\n",
       "      <th>run_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-0.25 | BETA-VAE beta 0.5 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=0.5] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-1 | BETA-VAE beta 0.1 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=0.1] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-0.75 | BETA-VAE beta 0.1 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=0.1] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-0.5 | BETA-VAE beta 0.1 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=0.1] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-0.25 | BETA-VAE beta 0.1 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=0.1] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-1 | BETA-VAE beta 0 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=0.0] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-0.75 | BETA-VAE beta 0 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=0.0] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-0.5 | BETA-VAE beta 0 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=0.0] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-0.25 | BETA-VAE beta 0 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=0.0] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-1 | BETA-VAE beta 1 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=1.0] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-0.75 | BETA-VAE beta 1 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=1.0] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-0.5 | BETA-VAE beta 1 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=1.0] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-0.25 | BETA-VAE beta 1 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=1.0] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-1 | BETA-VAE beta 0.5 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=0.5] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-0.75 | BETA-VAE beta 0.5 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=0.5] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer | drop-0.5 | BETA-VAE beta 0.5 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov drop yahoo-beta-vae) B-VAE[b=0.5] | ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer BETA-VAE beta 1.5 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov yahoo-beta-vae) B-VAE[b=1.5] | STRONG ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer BETA-VAE beta 1 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov yahoo-beta-vae) B-VAE[b=1.0] | STRONG ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer BETA-VAE beta 1.5 dec: Weak-M roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>weak_memory_distil_roberta_decoder</td>\n",
       "      <td>(29-nov yahoo-beta-vae) B-VAE[b=1.5] | WEAK | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer BETA-VAE beta 1 dec: Weak-M roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>weak_memory_distil_roberta_decoder</td>\n",
       "      <td>(29-nov yahoo-beta-vae) B-VAE[b=1.0] | WEAK | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer BETA-VAE beta 0.5 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov yahoo-beta-vae) B-VAE[b=0.5] | STRONG ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer BETA-VAE beta 0.5 dec: Weak-M roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>weak_memory_distil_roberta_decoder</td>\n",
       "      <td>(29-nov yahoo-beta-vae) B-VAE[b=0.5] | WEAK | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer BETA-VAE beta 0.1 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov yahoo-beta-vae) B-VAE[b=0.1] | STRONG ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer BETA-VAE beta 0.1 dec: Weak-M roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>weak_memory_distil_roberta_decoder</td>\n",
       "      <td>(29-nov yahoo-beta-vae) B-VAE[b=0.1] | WEAK | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer BETA-VAE beta 0 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov yahoo-beta-vae) B-VAE[b=0.0] | STRONG ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo_answer BETA-VAE beta 0 dec: Weak-M roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>yahoo_answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>weak_memory_distil_roberta_decoder</td>\n",
       "      <td>(29-nov yahoo-beta-vae) B-VAE[b=0.0] | WEAK | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptb BETA-VAE beta 1.5 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>ptb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov ptb-beta-vae) B-VAE[b=1.5] | STRONG | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptb BETA-VAE beta 1.5 dec: Weak-M roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>ptb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>weak_memory_distil_roberta_decoder</td>\n",
       "      <td>(29-nov ptb-beta-vae) B-VAE[b=1.5] | WEAK | Nz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptb BETA-VAE beta 1 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>ptb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov ptb-beta-vae) B-VAE[b=1.0] | STRONG | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptb BETA-VAE beta 1 dec: Weak-M roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>ptb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>weak_memory_distil_roberta_decoder</td>\n",
       "      <td>(29-nov ptb-beta-vae) B-VAE[b=1.0] | WEAK | Nz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptb BETA-VAE beta 0.5 dec: Weak-M roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>ptb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>weak_memory_distil_roberta_decoder</td>\n",
       "      <td>(29-nov ptb-beta-vae) B-VAE[b=0.5] | WEAK | Nz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptb BETA-VAE beta 0.5 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>ptb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov ptb-beta-vae) B-VAE[b=0.5] | STRONG | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptb BETA-VAE beta 0.1 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>ptb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov ptb-beta-vae) B-VAE[b=0.1] | STRONG | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptb BETA-VAE beta 0 dec: Strong roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>ptb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>strong_distil_roberta_decoder</td>\n",
       "      <td>(29-nov ptb-beta-vae) B-VAE[b=0.0] | STRONG | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptb BETA-VAE beta 0 dec: Weak-M roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>ptb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>weak_memory_distil_roberta_decoder</td>\n",
       "      <td>(29-nov ptb-beta-vae) B-VAE[b=0.0] | WEAK | Nz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptb BETA-VAE beta 0.1 dec: Weak-M roBERTa</th>\n",
       "      <td>BETA-VAE</td>\n",
       "      <td>ptb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>weak_memory_distil_roberta_decoder</td>\n",
       "      <td>(29-nov ptb-beta-vae) B-VAE[b=0.1] | WEAK | Nz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   objective       dataset  \\\n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0.5 de...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0.1 dec: ...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0.1 de...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0.1 dec...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0.1 de...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0 dec: St...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0 dec:...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0 dec: ...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0 dec:...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 1 dec: St...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 1 dec:...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 1 dec: ...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 1 dec:...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0.5 dec: ...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0.5 de...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0.5 dec...  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer BETA-VAE beta 1.5 dec: Strong roBERTa  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer BETA-VAE beta 1 dec: Strong roBERTa    BETA-VAE  yahoo_answer   \n",
       "yahoo_answer BETA-VAE beta 1.5 dec: Weak-M roBERTa  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer BETA-VAE beta 1 dec: Weak-M roBERTa    BETA-VAE  yahoo_answer   \n",
       "yahoo_answer BETA-VAE beta 0.5 dec: Strong roBERTa  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer BETA-VAE beta 0.5 dec: Weak-M roBERTa  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer BETA-VAE beta 0.1 dec: Strong roBERTa  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer BETA-VAE beta 0.1 dec: Weak-M roBERTa  BETA-VAE  yahoo_answer   \n",
       "yahoo_answer BETA-VAE beta 0 dec: Strong roBERTa    BETA-VAE  yahoo_answer   \n",
       "yahoo_answer BETA-VAE beta 0 dec: Weak-M roBERTa    BETA-VAE  yahoo_answer   \n",
       "ptb BETA-VAE beta 1.5 dec: Strong roBERTa           BETA-VAE           ptb   \n",
       "ptb BETA-VAE beta 1.5 dec: Weak-M roBERTa           BETA-VAE           ptb   \n",
       "ptb BETA-VAE beta 1 dec: Strong roBERTa             BETA-VAE           ptb   \n",
       "ptb BETA-VAE beta 1 dec: Weak-M roBERTa             BETA-VAE           ptb   \n",
       "ptb BETA-VAE beta 0.5 dec: Weak-M roBERTa           BETA-VAE           ptb   \n",
       "ptb BETA-VAE beta 0.5 dec: Strong roBERTa           BETA-VAE           ptb   \n",
       "ptb BETA-VAE beta 0.1 dec: Strong roBERTa           BETA-VAE           ptb   \n",
       "ptb BETA-VAE beta 0 dec: Strong roBERTa             BETA-VAE           ptb   \n",
       "ptb BETA-VAE beta 0 dec: Weak-M roBERTa             BETA-VAE           ptb   \n",
       "ptb BETA-VAE beta 0.1 dec: Weak-M roBERTa           BETA-VAE           ptb   \n",
       "\n",
       "                                                   l_rate  dropout beta_beta  \\\n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0.5 de...      0 0.250000  0.500000   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0.1 dec: ...      0        1  0.100000   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0.1 de...      0 0.750000  0.100000   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0.1 dec...      0 0.500000  0.100000   \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0.1 de...      0 0.250000  0.100000   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0 dec: St...      0        1         0   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0 dec:...      0 0.750000         0   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0 dec: ...      0 0.500000         0   \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0 dec:...      0 0.250000         0   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 1 dec: St...      0        1         1   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 1 dec:...      0 0.750000         1   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 1 dec: ...      0 0.500000         1   \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 1 dec:...      0 0.250000         1   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0.5 dec: ...      0        1  0.500000   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0.5 de...      0 0.750000  0.500000   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0.5 dec...      0 0.500000  0.500000   \n",
       "yahoo_answer BETA-VAE beta 1.5 dec: Strong roBERTa      0 0.000000  1.500000   \n",
       "yahoo_answer BETA-VAE beta 1 dec: Strong roBERTa        0 0.000000         1   \n",
       "yahoo_answer BETA-VAE beta 1.5 dec: Weak-M roBERTa      0 0.000000  1.500000   \n",
       "yahoo_answer BETA-VAE beta 1 dec: Weak-M roBERTa        0 0.000000         1   \n",
       "yahoo_answer BETA-VAE beta 0.5 dec: Strong roBERTa      0 0.000000  0.500000   \n",
       "yahoo_answer BETA-VAE beta 0.5 dec: Weak-M roBERTa      0 0.000000  0.500000   \n",
       "yahoo_answer BETA-VAE beta 0.1 dec: Strong roBERTa      0 0.000000  0.100000   \n",
       "yahoo_answer BETA-VAE beta 0.1 dec: Weak-M roBERTa      0 0.000000  0.100000   \n",
       "yahoo_answer BETA-VAE beta 0 dec: Strong roBERTa        0 0.000000         0   \n",
       "yahoo_answer BETA-VAE beta 0 dec: Weak-M roBERTa        0 0.000000         0   \n",
       "ptb BETA-VAE beta 1.5 dec: Strong roBERTa               0 0.000000  1.500000   \n",
       "ptb BETA-VAE beta 1.5 dec: Weak-M roBERTa               0 0.000000  1.500000   \n",
       "ptb BETA-VAE beta 1 dec: Strong roBERTa                 0 0.000000         1   \n",
       "ptb BETA-VAE beta 1 dec: Weak-M roBERTa                 0 0.000000         1   \n",
       "ptb BETA-VAE beta 0.5 dec: Weak-M roBERTa               0 0.000000  0.500000   \n",
       "ptb BETA-VAE beta 0.5 dec: Strong roBERTa               0 0.000000  0.500000   \n",
       "ptb BETA-VAE beta 0.1 dec: Strong roBERTa               0 0.000000  0.100000   \n",
       "ptb BETA-VAE beta 0 dec: Strong roBERTa                 0 0.000000         0   \n",
       "ptb BETA-VAE beta 0 dec: Weak-M roBERTa                 0 0.000000         0   \n",
       "ptb BETA-VAE beta 0.1 dec: Weak-M roBERTa               0 0.000000  0.100000   \n",
       "\n",
       "                                                   free_bits mdr_value l_mmd  \\\n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0.5 de...         0         0     0   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0.1 dec: ...         0         0     0   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0.1 de...         0         0     0   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0.1 dec...         0         0     0   \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0.1 de...         0         0     0   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0 dec: St...         0         0     0   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0 dec:...         0         0     0   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0 dec: ...         0         0     0   \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0 dec:...         0         0     0   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 1 dec: St...         0         0     0   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 1 dec:...         0         0     0   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 1 dec: ...         0         0     0   \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 1 dec:...         0         0     0   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0.5 dec: ...         0         0     0   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0.5 de...         0         0     0   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0.5 dec...         0         0     0   \n",
       "yahoo_answer BETA-VAE beta 1.5 dec: Strong roBERTa         0         0     0   \n",
       "yahoo_answer BETA-VAE beta 1 dec: Strong roBERTa           0         0     0   \n",
       "yahoo_answer BETA-VAE beta 1.5 dec: Weak-M roBERTa         0         0     0   \n",
       "yahoo_answer BETA-VAE beta 1 dec: Weak-M roBERTa           0         0     0   \n",
       "yahoo_answer BETA-VAE beta 0.5 dec: Strong roBERTa         0         0     0   \n",
       "yahoo_answer BETA-VAE beta 0.5 dec: Weak-M roBERTa         0         0     0   \n",
       "yahoo_answer BETA-VAE beta 0.1 dec: Strong roBERTa         0         0     0   \n",
       "yahoo_answer BETA-VAE beta 0.1 dec: Weak-M roBERTa         0         0     0   \n",
       "yahoo_answer BETA-VAE beta 0 dec: Strong roBERTa           0         0     0   \n",
       "yahoo_answer BETA-VAE beta 0 dec: Weak-M roBERTa           0         0     0   \n",
       "ptb BETA-VAE beta 1.5 dec: Strong roBERTa                  0         0     0   \n",
       "ptb BETA-VAE beta 1.5 dec: Weak-M roBERTa                  0         0     0   \n",
       "ptb BETA-VAE beta 1 dec: Strong roBERTa                    0         0     0   \n",
       "ptb BETA-VAE beta 1 dec: Weak-M roBERTa                    0         0     0   \n",
       "ptb BETA-VAE beta 0.5 dec: Weak-M roBERTa                  0         0     0   \n",
       "ptb BETA-VAE beta 0.5 dec: Strong roBERTa                  0         0     0   \n",
       "ptb BETA-VAE beta 0.1 dec: Strong roBERTa                  0         0     0   \n",
       "ptb BETA-VAE beta 0 dec: Strong roBERTa                    0         0     0   \n",
       "ptb BETA-VAE beta 0 dec: Weak-M roBERTa                    0         0     0   \n",
       "ptb BETA-VAE beta 0.1 dec: Weak-M roBERTa                  0         0     0   \n",
       "\n",
       "                                                                               decoder  \\\n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0.5 de...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0.1 dec: ...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0.1 de...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0.1 dec...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0.1 de...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0 dec: St...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0 dec:...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0 dec: ...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0 dec:...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 1 dec: St...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 1 dec:...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 1 dec: ...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 1 dec:...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0.5 dec: ...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0.5 de...       strong_distil_roberta_decoder   \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0.5 dec...       strong_distil_roberta_decoder   \n",
       "yahoo_answer BETA-VAE beta 1.5 dec: Strong roBERTa       strong_distil_roberta_decoder   \n",
       "yahoo_answer BETA-VAE beta 1 dec: Strong roBERTa         strong_distil_roberta_decoder   \n",
       "yahoo_answer BETA-VAE beta 1.5 dec: Weak-M roBERTa  weak_memory_distil_roberta_decoder   \n",
       "yahoo_answer BETA-VAE beta 1 dec: Weak-M roBERTa    weak_memory_distil_roberta_decoder   \n",
       "yahoo_answer BETA-VAE beta 0.5 dec: Strong roBERTa       strong_distil_roberta_decoder   \n",
       "yahoo_answer BETA-VAE beta 0.5 dec: Weak-M roBERTa  weak_memory_distil_roberta_decoder   \n",
       "yahoo_answer BETA-VAE beta 0.1 dec: Strong roBERTa       strong_distil_roberta_decoder   \n",
       "yahoo_answer BETA-VAE beta 0.1 dec: Weak-M roBERTa  weak_memory_distil_roberta_decoder   \n",
       "yahoo_answer BETA-VAE beta 0 dec: Strong roBERTa         strong_distil_roberta_decoder   \n",
       "yahoo_answer BETA-VAE beta 0 dec: Weak-M roBERTa    weak_memory_distil_roberta_decoder   \n",
       "ptb BETA-VAE beta 1.5 dec: Strong roBERTa                strong_distil_roberta_decoder   \n",
       "ptb BETA-VAE beta 1.5 dec: Weak-M roBERTa           weak_memory_distil_roberta_decoder   \n",
       "ptb BETA-VAE beta 1 dec: Strong roBERTa                  strong_distil_roberta_decoder   \n",
       "ptb BETA-VAE beta 1 dec: Weak-M roBERTa             weak_memory_distil_roberta_decoder   \n",
       "ptb BETA-VAE beta 0.5 dec: Weak-M roBERTa           weak_memory_distil_roberta_decoder   \n",
       "ptb BETA-VAE beta 0.5 dec: Strong roBERTa                strong_distil_roberta_decoder   \n",
       "ptb BETA-VAE beta 0.1 dec: Strong roBERTa                strong_distil_roberta_decoder   \n",
       "ptb BETA-VAE beta 0 dec: Strong roBERTa                  strong_distil_roberta_decoder   \n",
       "ptb BETA-VAE beta 0 dec: Weak-M roBERTa             weak_memory_distil_roberta_decoder   \n",
       "ptb BETA-VAE beta 0.1 dec: Weak-M roBERTa           weak_memory_distil_roberta_decoder   \n",
       "\n",
       "                                                                                             run_name  \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0.5 de...  (29-nov drop yahoo-beta-vae) B-VAE[b=0.5] | ST...  \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0.1 dec: ...  (29-nov drop yahoo-beta-vae) B-VAE[b=0.1] | ST...  \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0.1 de...  (29-nov drop yahoo-beta-vae) B-VAE[b=0.1] | ST...  \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0.1 dec...  (29-nov drop yahoo-beta-vae) B-VAE[b=0.1] | ST...  \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0.1 de...  (29-nov drop yahoo-beta-vae) B-VAE[b=0.1] | ST...  \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0 dec: St...  (29-nov drop yahoo-beta-vae) B-VAE[b=0.0] | ST...  \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0 dec:...  (29-nov drop yahoo-beta-vae) B-VAE[b=0.0] | ST...  \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0 dec: ...  (29-nov drop yahoo-beta-vae) B-VAE[b=0.0] | ST...  \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 0 dec:...  (29-nov drop yahoo-beta-vae) B-VAE[b=0.0] | ST...  \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 1 dec: St...  (29-nov drop yahoo-beta-vae) B-VAE[b=1.0] | ST...  \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 1 dec:...  (29-nov drop yahoo-beta-vae) B-VAE[b=1.0] | ST...  \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 1 dec: ...  (29-nov drop yahoo-beta-vae) B-VAE[b=1.0] | ST...  \n",
       "yahoo_answer | drop-0.25 | BETA-VAE beta 1 dec:...  (29-nov drop yahoo-beta-vae) B-VAE[b=1.0] | ST...  \n",
       "yahoo_answer | drop-1 | BETA-VAE beta 0.5 dec: ...  (29-nov drop yahoo-beta-vae) B-VAE[b=0.5] | ST...  \n",
       "yahoo_answer | drop-0.75 | BETA-VAE beta 0.5 de...  (29-nov drop yahoo-beta-vae) B-VAE[b=0.5] | ST...  \n",
       "yahoo_answer | drop-0.5 | BETA-VAE beta 0.5 dec...  (29-nov drop yahoo-beta-vae) B-VAE[b=0.5] | ST...  \n",
       "yahoo_answer BETA-VAE beta 1.5 dec: Strong roBERTa  (29-nov yahoo-beta-vae) B-VAE[b=1.5] | STRONG ...  \n",
       "yahoo_answer BETA-VAE beta 1 dec: Strong roBERTa    (29-nov yahoo-beta-vae) B-VAE[b=1.0] | STRONG ...  \n",
       "yahoo_answer BETA-VAE beta 1.5 dec: Weak-M roBERTa  (29-nov yahoo-beta-vae) B-VAE[b=1.5] | WEAK | ...  \n",
       "yahoo_answer BETA-VAE beta 1 dec: Weak-M roBERTa    (29-nov yahoo-beta-vae) B-VAE[b=1.0] | WEAK | ...  \n",
       "yahoo_answer BETA-VAE beta 0.5 dec: Strong roBERTa  (29-nov yahoo-beta-vae) B-VAE[b=0.5] | STRONG ...  \n",
       "yahoo_answer BETA-VAE beta 0.5 dec: Weak-M roBERTa  (29-nov yahoo-beta-vae) B-VAE[b=0.5] | WEAK | ...  \n",
       "yahoo_answer BETA-VAE beta 0.1 dec: Strong roBERTa  (29-nov yahoo-beta-vae) B-VAE[b=0.1] | STRONG ...  \n",
       "yahoo_answer BETA-VAE beta 0.1 dec: Weak-M roBERTa  (29-nov yahoo-beta-vae) B-VAE[b=0.1] | WEAK | ...  \n",
       "yahoo_answer BETA-VAE beta 0 dec: Strong roBERTa    (29-nov yahoo-beta-vae) B-VAE[b=0.0] | STRONG ...  \n",
       "yahoo_answer BETA-VAE beta 0 dec: Weak-M roBERTa    (29-nov yahoo-beta-vae) B-VAE[b=0.0] | WEAK | ...  \n",
       "ptb BETA-VAE beta 1.5 dec: Strong roBERTa           (29-nov ptb-beta-vae) B-VAE[b=1.5] | STRONG | ...  \n",
       "ptb BETA-VAE beta 1.5 dec: Weak-M roBERTa           (29-nov ptb-beta-vae) B-VAE[b=1.5] | WEAK | Nz...  \n",
       "ptb BETA-VAE beta 1 dec: Strong roBERTa             (29-nov ptb-beta-vae) B-VAE[b=1.0] | STRONG | ...  \n",
       "ptb BETA-VAE beta 1 dec: Weak-M roBERTa             (29-nov ptb-beta-vae) B-VAE[b=1.0] | WEAK | Nz...  \n",
       "ptb BETA-VAE beta 0.5 dec: Weak-M roBERTa           (29-nov ptb-beta-vae) B-VAE[b=0.5] | WEAK | Nz...  \n",
       "ptb BETA-VAE beta 0.5 dec: Strong roBERTa           (29-nov ptb-beta-vae) B-VAE[b=0.5] | STRONG | ...  \n",
       "ptb BETA-VAE beta 0.1 dec: Strong roBERTa           (29-nov ptb-beta-vae) B-VAE[b=0.1] | STRONG | ...  \n",
       "ptb BETA-VAE beta 0 dec: Strong roBERTa             (29-nov ptb-beta-vae) B-VAE[b=0.0] | STRONG | ...  \n",
       "ptb BETA-VAE beta 0 dec: Weak-M roBERTa             (29-nov ptb-beta-vae) B-VAE[b=0.0] | WEAK | Nz...  \n",
       "ptb BETA-VAE beta 0.1 dec: Weak-M roBERTa           (29-nov ptb-beta-vae) B-VAE[b=0.1] | WEAK | Nz...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixes = [\"(29-nov ptb-beta-vae)\", \"(29-nov yahoo-beta-vae)\", \"(29-nov drop yahoo-beta-vae)\"]\n",
    "overview_df = make_run_overview_df(prefixes, add_data_group=False)\n",
    "overview_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yahoo_answer | drop-0.25 | BETA-VAE beta 0.5 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 5.34 | total_loss: 161.39 | mmd: 0.00 | elbo: -166.73 | distortion: 156.05 | kl_prior_post: 10.69 | mean_mean: -0.00 | std_across_x_mean: 0.31 | std_across_z_mean: 0.35 | mean_scale: 0.92 | std_across_x_scale: 0.04 | std_across_z_scale: 0.10 | \n",
      "yahoo_answer | drop-1 | BETA-VAE beta 0.1 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 5.07 | total_loss: 271.08 | mmd: 0.00 | elbo: -316.72 | distortion: 266.01 | kl_prior_post: 50.71 | mean_mean: -0.00 | std_across_x_mean: 0.49 | std_across_z_mean: 0.58 | mean_scale: 0.77 | std_across_x_scale: 0.05 | std_across_z_scale: 0.26 | \n",
      "yahoo_answer | drop-0.75 | BETA-VAE beta 0.1 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 8.82 | total_loss: 162.54 | mmd: 0.00 | elbo: -241.88 | distortion: 153.73 | kl_prior_post: 88.15 | mean_mean: -0.01 | std_across_x_mean: 0.75 | std_across_z_mean: 0.78 | mean_scale: 0.56 | std_across_x_scale: 0.10 | std_across_z_scale: 0.21 | \n",
      "yahoo_answer | drop-0.5 | BETA-VAE beta 0.1 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 9.53 | total_loss: 150.79 | mmd: 0.00 | elbo: -236.51 | distortion: 141.26 | kl_prior_post: 95.25 | mean_mean: 0.02 | std_across_x_mean: 0.77 | std_across_z_mean: 0.79 | mean_scale: 0.51 | std_across_x_scale: 0.12 | std_across_z_scale: 0.18 | \n",
      "yahoo_answer | drop-0.25 | BETA-VAE beta 0.1 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 9.43 | total_loss: 143.58 | mmd: 0.00 | elbo: -228.47 | distortion: 134.15 | kl_prior_post: 94.31 | mean_mean: -0.00 | std_across_x_mean: 0.78 | std_across_z_mean: 0.80 | mean_scale: 0.51 | std_across_x_scale: 0.12 | std_across_z_scale: 0.16 | \n",
      "yahoo_answer | drop-1 | BETA-VAE beta 0 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 0.00 | total_loss: 278.56 | mmd: 0.00 | elbo: -694.66 | distortion: 278.56 | kl_prior_post: 416.10 | mean_mean: 0.02 | std_across_x_mean: 0.71 | std_across_z_mean: 1.13 | mean_scale: 0.06 | std_across_x_scale: 0.04 | std_across_z_scale: 0.03 | \n",
      "yahoo_answer | drop-0.75 | BETA-VAE beta 0 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 0.00 | total_loss: 147.74 | mmd: 0.00 | elbo: -858.23 | distortion: 147.74 | kl_prior_post: 710.49 | mean_mean: -0.01 | std_across_x_mean: 0.64 | std_across_z_mean: 0.76 | mean_scale: 0.01 | std_across_x_scale: 0.01 | std_across_z_scale: 0.00 | \n",
      "yahoo_answer | drop-0.5 | BETA-VAE beta 0 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 0.00 | total_loss: 132.45 | mmd: 0.00 | elbo: -844.83 | distortion: 132.45 | kl_prior_post: 712.38 | mean_mean: -0.03 | std_across_x_mean: 0.59 | std_across_z_mean: 0.70 | mean_scale: 0.01 | std_across_x_scale: 0.01 | std_across_z_scale: 0.00 | \n",
      "yahoo_answer | drop-0.25 | BETA-VAE beta 0 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 0.00 | total_loss: 121.73 | mmd: 0.00 | elbo: -803.50 | distortion: 121.73 | kl_prior_post: 681.76 | mean_mean: -0.02 | std_across_x_mean: 0.58 | std_across_z_mean: 0.71 | mean_scale: 0.01 | std_across_x_scale: 0.01 | std_across_z_scale: 0.00 | \n",
      "yahoo_answer | drop-1 | BETA-VAE beta 1 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 3.16 | total_loss: 285.03 | mmd: 0.00 | elbo: -285.03 | distortion: 281.87 | kl_prior_post: 3.16 | mean_mean: 0.00 | std_across_x_mean: 0.04 | std_across_z_mean: 0.12 | mean_scale: 0.99 | std_across_x_scale: 0.02 | std_across_z_scale: 0.09 | \n",
      "yahoo_answer | drop-0.75 | BETA-VAE beta 1 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 7.35 | total_loss: 178.73 | mmd: 0.00 | elbo: -178.73 | distortion: 171.38 | kl_prior_post: 7.35 | mean_mean: -0.00 | std_across_x_mean: 0.22 | std_across_z_mean: 0.26 | mean_scale: 0.96 | std_across_x_scale: 0.02 | std_across_z_scale: 0.11 | \n",
      "yahoo_answer | drop-0.5 | BETA-VAE beta 1 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 3.23 | total_loss: 168.55 | mmd: 0.00 | elbo: -168.55 | distortion: 165.32 | kl_prior_post: 3.23 | mean_mean: -0.00 | std_across_x_mean: 0.19 | std_across_z_mean: 0.20 | mean_scale: 0.97 | std_across_x_scale: 0.02 | std_across_z_scale: 0.04 | \n",
      "yahoo_answer | drop-0.25 | BETA-VAE beta 1 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 0.50 | total_loss: 163.70 | mmd: 0.00 | elbo: -163.70 | distortion: 163.20 | kl_prior_post: 0.50 | mean_mean: 0.00 | std_across_x_mean: 0.08 | std_across_z_mean: 0.08 | mean_scale: 0.99 | std_across_x_scale: 0.01 | std_across_z_scale: 0.01 | \n",
      "yahoo_answer | drop-1 | BETA-VAE beta 0.5 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 4.44 | total_loss: 280.92 | mmd: 0.00 | elbo: -285.36 | distortion: 276.48 | kl_prior_post: 8.88 | mean_mean: 0.00 | std_across_x_mean: 0.18 | std_across_z_mean: 0.26 | mean_scale: 0.96 | std_across_x_scale: 0.02 | std_across_z_scale: 0.13 | \n",
      "yahoo_answer | drop-0.75 | BETA-VAE beta 0.5 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 9.59 | total_loss: 175.84 | mmd: 0.00 | elbo: -185.44 | distortion: 166.25 | kl_prior_post: 19.19 | mean_mean: -0.00 | std_across_x_mean: 0.34 | std_across_z_mean: 0.41 | mean_scale: 0.89 | std_across_x_scale: 0.04 | std_across_z_scale: 0.17 | \n",
      "yahoo_answer | drop-0.5 | BETA-VAE beta 0.5 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 7.99 | total_loss: 166.17 | mmd: 0.00 | elbo: -174.16 | distortion: 158.18 | kl_prior_post: 15.98 | mean_mean: 0.00 | std_across_x_mean: 0.36 | std_across_z_mean: 0.40 | mean_scale: 0.90 | std_across_x_scale: 0.05 | std_across_z_scale: 0.13 | \n",
      "yahoo_answer BETA-VAE beta 1.5 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 0.02 | total_loss: 159.71 | mmd: 0.00 | elbo: -159.71 | distortion: 159.69 | kl_prior_post: 0.01 | mean_mean: -0.00 | std_across_x_mean: 0.01 | std_across_z_mean: 0.01 | mean_scale: 0.99 | std_across_x_scale: 0.00 | std_across_z_scale: 0.00 | \n",
      "yahoo_answer BETA-VAE beta 1 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 0.05 | total_loss: 159.73 | mmd: 0.00 | elbo: -159.73 | distortion: 159.68 | kl_prior_post: 0.05 | mean_mean: 0.00 | std_across_x_mean: 0.02 | std_across_z_mean: 0.02 | mean_scale: 0.99 | std_across_x_scale: 0.01 | std_across_z_scale: 0.00 | \n",
      "yahoo_answer BETA-VAE beta 1.5 dec: Weak-M roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_p_l_z: -4.01 | log_p_x_z (without l): -266.66 | beta_kl: 5.30 | total_loss: 275.97 | mmd: 0.00 | elbo: -274.20 | distortion: 270.67 | kl_prior_post: 3.53 | mean_mean: 0.00 | std_across_x_mean: 0.04 | std_across_z_mean: 0.11 | mean_scale: 0.99 | std_across_x_scale: 0.01 | std_across_z_scale: 0.10 | \n",
      "yahoo_answer BETA-VAE beta 1 dec: Weak-M roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_p_l_z: -3.74 | log_p_x_z (without l): -259.89 | beta_kl: 6.37 | total_loss: 270.01 | mmd: 0.00 | elbo: -270.01 | distortion: 263.63 | kl_prior_post: 6.37 | mean_mean: 0.00 | std_across_x_mean: 0.08 | std_across_z_mean: 0.19 | mean_scale: 0.98 | std_across_x_scale: 0.01 | std_across_z_scale: 0.13 | \n",
      "yahoo_answer BETA-VAE beta 0.5 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 1.95 | total_loss: 159.14 | mmd: 0.00 | elbo: -161.09 | distortion: 157.19 | kl_prior_post: 3.90 | mean_mean: 0.00 | std_across_x_mean: 0.22 | std_across_z_mean: 0.23 | mean_scale: 0.96 | std_across_x_scale: 0.03 | std_across_z_scale: 0.03 | \n",
      "yahoo_answer BETA-VAE beta 0.5 dec: Weak-M roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_p_l_z: -3.11 | log_p_x_z (without l): -250.36 | beta_kl: 7.02 | total_loss: 260.49 | mmd: 0.00 | elbo: -267.50 | distortion: 253.47 | kl_prior_post: 14.03 | mean_mean: 0.00 | std_across_x_mean: 0.23 | std_across_z_mean: 0.31 | mean_scale: 0.94 | std_across_x_scale: 0.03 | std_across_z_scale: 0.17 | \n",
      "yahoo_answer BETA-VAE beta 0.1 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 8.54 | total_loss: 139.85 | mmd: 0.00 | elbo: -216.72 | distortion: 131.30 | kl_prior_post: 85.42 | mean_mean: -0.01 | std_across_x_mean: 0.76 | std_across_z_mean: 0.78 | mean_scale: 0.53 | std_across_x_scale: 0.12 | std_across_z_scale: 0.15 | \n",
      "yahoo_answer BETA-VAE beta 0.1 dec: Weak-M roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_p_l_z: -2.56 | log_p_x_z (without l): -225.18 | beta_kl: 6.38 | total_loss: 234.12 | mmd: 0.00 | elbo: -291.55 | distortion: 227.74 | kl_prior_post: 63.80 | mean_mean: 0.01 | std_across_x_mean: 0.58 | std_across_z_mean: 0.65 | mean_scale: 0.72 | std_across_x_scale: 0.06 | std_across_z_scale: 0.28 | \n",
      "yahoo_answer BETA-VAE beta 0 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 0.00 | total_loss: 115.74 | mmd: 0.00 | elbo: -815.19 | distortion: 115.74 | kl_prior_post: 699.45 | mean_mean: -0.01 | std_across_x_mean: 0.54 | std_across_z_mean: 0.62 | mean_scale: 0.00 | std_across_x_scale: 0.01 | std_across_z_scale: 0.00 | \n",
      "yahoo_answer BETA-VAE beta 0 dec: Weak-M roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_p_l_z: -2.18 | log_p_x_z (without l): -192.11 | beta_kl: 0.00 | total_loss: 194.29 | mmd: 0.00 | elbo: -963.95 | distortion: 194.29 | kl_prior_post: 769.66 | mean_mean: -0.00 | std_across_x_mean: 0.69 | std_across_z_mean: 0.85 | mean_scale: 0.00 | std_across_x_scale: 0.01 | std_across_z_scale: 0.00 | \n",
      "ptb BETA-VAE beta 1.5 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 0.01 | total_loss: 90.78 | mmd: 0.00 | elbo: -90.78 | distortion: 90.78 | kl_prior_post: 0.00 | mean_mean: -0.00 | std_across_x_mean: 0.00 | std_across_z_mean: 0.00 | mean_scale: 1.00 | std_across_x_scale: 0.00 | std_across_z_scale: 0.00 | \n",
      "ptb BETA-VAE beta 1.5 dec: Weak-M roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_p_l_z: -4.20 | log_p_x_z (without l): -152.04 | beta_kl: 3.12 | total_loss: 159.36 | mmd: 0.00 | elbo: -158.32 | distortion: 156.24 | kl_prior_post: 2.08 | mean_mean: -0.00 | std_across_x_mean: 0.06 | std_across_z_mean: 0.11 | mean_scale: 0.99 | std_across_x_scale: 0.00 | std_across_z_scale: 0.07 | \n",
      "ptb BETA-VAE beta 1 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 0.02 | total_loss: 90.71 | mmd: 0.00 | elbo: -90.71 | distortion: 90.70 | kl_prior_post: 0.02 | mean_mean: 0.00 | std_across_x_mean: 0.01 | std_across_z_mean: 0.01 | mean_scale: 0.99 | std_across_x_scale: 0.00 | std_across_z_scale: 0.00 | \n",
      "ptb BETA-VAE beta 1 dec: Weak-M roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_p_l_z: -4.21 | log_p_x_z (without l): -147.61 | beta_kl: 3.90 | total_loss: 155.72 | mmd: 0.00 | elbo: -155.72 | distortion: 151.82 | kl_prior_post: 3.90 | mean_mean: -0.00 | std_across_x_mean: 0.07 | std_across_z_mean: 0.15 | mean_scale: 0.98 | std_across_x_scale: 0.01 | std_across_z_scale: 0.10 | \n",
      "ptb BETA-VAE beta 0.5 dec: Weak-M roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_p_l_z: -4.18 | log_p_x_z (without l): -140.51 | beta_kl: 4.49 | total_loss: 149.18 | mmd: 0.00 | elbo: -153.67 | distortion: 144.69 | kl_prior_post: 8.98 | mean_mean: -0.01 | std_across_x_mean: 0.06 | std_across_z_mean: 0.20 | mean_scale: 0.97 | std_across_x_scale: 0.01 | std_across_z_scale: 0.16 | \n",
      "ptb BETA-VAE beta 0.5 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 1.06 | total_loss: 90.38 | mmd: 0.00 | elbo: -91.44 | distortion: 89.32 | kl_prior_post: 2.12 | mean_mean: -0.00 | std_across_x_mean: 0.13 | std_across_z_mean: 0.16 | mean_scale: 0.98 | std_across_x_scale: 0.02 | std_across_z_scale: 0.04 | \n",
      "ptb BETA-VAE beta 0.1 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 6.91 | total_loss: 74.56 | mmd: 0.00 | elbo: -136.76 | distortion: 67.65 | kl_prior_post: 69.11 | mean_mean: 0.00 | std_across_x_mean: 0.67 | std_across_z_mean: 0.73 | mean_scale: 0.62 | std_across_x_scale: 0.08 | std_across_z_scale: 0.19 | \n",
      "ptb BETA-VAE beta 0 dec: Strong roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_kl: 0.00 | total_loss: 53.51 | mmd: 0.00 | elbo: -735.47 | distortion: 53.51 | kl_prior_post: 681.95 | mean_mean: -0.03 | std_across_x_mean: 0.58 | std_across_z_mean: 0.69 | mean_scale: 0.01 | std_across_x_scale: 0.01 | std_across_z_scale: 0.00 | \n",
      "ptb BETA-VAE beta 0 dec: Weak-M roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_p_l_z: -3.17 | log_p_x_z (without l): -84.56 | beta_kl: 0.00 | total_loss: 87.73 | mmd: 0.00 | elbo: -919.74 | distortion: 87.73 | kl_prior_post: 832.01 | mean_mean: -0.00 | std_across_x_mean: 0.63 | std_across_z_mean: 0.84 | mean_scale: 0.00 | std_across_x_scale: 0.01 | std_across_z_scale: 0.00 | \n",
      "ptb BETA-VAE beta 0.1 dec: Weak-M roBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_p_l_z: -3.96 | log_p_x_z (without l): -116.16 | beta_kl: 4.90 | total_loss: 125.02 | mmd: 0.00 | elbo: -169.15 | distortion: 120.12 | kl_prior_post: 49.03 | mean_mean: 0.01 | std_across_x_mean: 0.38 | std_across_z_mean: 0.51 | mean_scale: 0.80 | std_across_x_scale: 0.04 | std_across_z_scale: 0.29 | \n",
      " 9/10\r"
     ]
    }
   ],
   "source": [
    "all_conditional_samples = dict()\n",
    "# all_conditional_samples = torch.load(\"all_conditional_samples_so_far.pt\")\n",
    "\n",
    "for clean_name, row in overview_df.iterrows():\n",
    "    print(clean_name)\n",
    "    \n",
    "    if clean_name in all_conditional_samples:\n",
    "        continue\n",
    "    \n",
    "    checkpoint_path = f\"{CHECKPOINT_DIR}/{row.run_name}.pt\"\n",
    "    \n",
    "    vae_model = load_checkpoint_model_for_eval(checkpoint_path, \n",
    "                                               map_location=DEVICE, \n",
    "                                               return_args=False).to(DEVICE)\n",
    "    \n",
    "    if \"ptb\" in clean_name:\n",
    "        data_loader = all_loaders[\"ptb\"]\n",
    "    else:\n",
    "        data_loader = all_loaders[\"yahoo\"]\n",
    "    \n",
    "    all_conditional_samples[clean_name] = []\n",
    "        \n",
    "    for i, (input_ids, attention_mask) in enumerate(data_loader):\n",
    "        print(f\"{i:2d}/{len(data_loader)}\", end=\"\\r\")\n",
    "\n",
    "        x_in = (input_ids.to(DEVICE), attention_mask.to(DEVICE))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, z_post = vae_model.inf_model(x_in=x_in, n_samples=1)\n",
    "\n",
    "            # Conditional sampling\n",
    "            # [Sx, Sz, B, L] -> [B, L]\n",
    "            conditional_sample_x = vae_model.gen_model.sample_generative_model(z=z_post,\n",
    "                                                                               Sx=1, Sz=1, return_z=False,\n",
    "                                                                               device=DEVICE).squeeze(0).squeeze(0)\n",
    "\n",
    "        all_conditional_samples[clean_name].append(conditional_sample_x)\n",
    "\n",
    "    all_conditional_samples[clean_name] = torch.cat(all_conditional_samples[clean_name]).cpu()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_conditional_samples, \"all_conditional_samples_so_far.pt\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 yahoo_answer | drop-0.25 | BETA-VAE beta 0.5 dec: Strong roBERTa\n",
      "1 yahoo_answer | drop-1 | BETA-VAE beta 0.1 dec: Strong roBERTa\n",
      "2 yahoo_answer | drop-0.75 | BETA-VAE beta 0.1 dec: Strong roBERTa\n",
      "3 yahoo_answer | drop-0.5 | BETA-VAE beta 0.1 dec: Strong roBERTa\n",
      "4 yahoo_answer | drop-0.25 | BETA-VAE beta 0.1 dec: Strong roBERTa\n",
      "5 yahoo_answer | drop-1 | BETA-VAE beta 0 dec: Strong roBERTa\n",
      "6 yahoo_answer | drop-0.75 | BETA-VAE beta 0 dec: Strong roBERTa\n",
      "7 yahoo_answer | drop-0.5 | BETA-VAE beta 0 dec: Strong roBERTa\n",
      "8 yahoo_answer | drop-0.25 | BETA-VAE beta 0 dec: Strong roBERTa\n",
      "9 yahoo_answer | drop-1 | BETA-VAE beta 1 dec: Strong roBERTa\n",
      "10 yahoo_answer | drop-0.75 | BETA-VAE beta 1 dec: Strong roBERTa\n",
      "11 yahoo_answer | drop-0.5 | BETA-VAE beta 1 dec: Strong roBERTa\n",
      "12 yahoo_answer | drop-0.25 | BETA-VAE beta 1 dec: Strong roBERTa\n",
      "13 yahoo_answer | drop-1 | BETA-VAE beta 0.5 dec: Strong roBERTa\n",
      "14 yahoo_answer | drop-0.75 | BETA-VAE beta 0.5 dec: Strong roBERTa\n",
      "15 yahoo_answer | drop-0.5 | BETA-VAE beta 0.5 dec: Strong roBERTa\n",
      "16 yahoo_answer BETA-VAE beta 1.5 dec: Strong roBERTa\n",
      "17 yahoo_answer BETA-VAE beta 1 dec: Strong roBERTa\n",
      "18 yahoo_answer BETA-VAE beta 1.5 dec: Weak-M roBERTa\n",
      "19 yahoo_answer BETA-VAE beta 1 dec: Weak-M roBERTa\n",
      "20 yahoo_answer BETA-VAE beta 0.5 dec: Strong roBERTa\n",
      "21 yahoo_answer BETA-VAE beta 0.5 dec: Weak-M roBERTa\n",
      "22 yahoo_answer BETA-VAE beta 0.1 dec: Strong roBERTa\n",
      "23 yahoo_answer BETA-VAE beta 0.1 dec: Weak-M roBERTa\n",
      "24 yahoo_answer BETA-VAE beta 0 dec: Strong roBERTa\n",
      "25 yahoo_answer BETA-VAE beta 0 dec: Weak-M roBERTa\n",
      "26 ptb BETA-VAE beta 1.5 dec: Strong roBERTa\n",
      "27 ptb BETA-VAE beta 1.5 dec: Weak-M roBERTa\n",
      "28 ptb BETA-VAE beta 1 dec: Strong roBERTa\n",
      "29 ptb BETA-VAE beta 1 dec: Weak-M roBERTa\n",
      "30 ptb BETA-VAE beta 0.5 dec: Weak-M roBERTa\n",
      "31 ptb BETA-VAE beta 0.5 dec: Strong roBERTa\n",
      "32 ptb BETA-VAE beta 0.1 dec: Strong roBERTa\n",
      "33 ptb BETA-VAE beta 0 dec: Strong roBERTa\n",
      "34 ptb BETA-VAE beta 0 dec: Weak-M roBERTa\n",
      "35 ptb BETA-VAE beta 0.1 dec: Weak-M roBERTa\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "max_samples = -1\n",
    "\n",
    "all_unconditional_samples = dict()\n",
    "\n",
    "for i, (clean_name, row) in enumerate(overview_df.iterrows()):\n",
    "    print(i, clean_name)\n",
    "    \n",
    "    save_dir = f\"{ANALYSIS_DIR}/{row.run_name}\"\n",
    "    sample_file = f\"{save_dir}/{SAMPLE_FILE}\"\n",
    "    \n",
    "    if os.path.exists(sample_file):\n",
    "        samples = torch.load(sample_file)\n",
    "        all_unconditional_samples[clean_name] = samples[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yahoo_answer | drop-0.25 | BETA-VAE beta 0.5 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-1 | BETA-VAE beta 0.1 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-0.75 | BETA-VAE beta 0.1 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-0.5 | BETA-VAE beta 0.1 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-0.25 | BETA-VAE beta 0.1 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-1 | BETA-VAE beta 0 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-0.75 | BETA-VAE beta 0 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-0.5 | BETA-VAE beta 0 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-0.25 | BETA-VAE beta 0 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-1 | BETA-VAE beta 1 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-0.75 | BETA-VAE beta 1 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-0.5 | BETA-VAE beta 1 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-0.25 | BETA-VAE beta 1 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-1 | BETA-VAE beta 0.5 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-0.75 | BETA-VAE beta 0.5 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer | drop-0.5 | BETA-VAE beta 0.5 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer BETA-VAE beta 1.5 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer BETA-VAE beta 1 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer BETA-VAE beta 1.5 dec: Weak-M roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer BETA-VAE beta 1 dec: Weak-M roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer BETA-VAE beta 0.5 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer BETA-VAE beta 0.5 dec: Weak-M roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer BETA-VAE beta 0.1 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer BETA-VAE beta 0.1 dec: Weak-M roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer BETA-VAE beta 0 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "yahoo_answer BETA-VAE beta 0 dec: Weak-M roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "ptb BETA-VAE beta 1.5 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "ptb BETA-VAE beta 1.5 dec: Weak-M roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "ptb BETA-VAE beta 1 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "ptb BETA-VAE beta 1 dec: Weak-M roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "ptb BETA-VAE beta 0.5 dec: Weak-M roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "ptb BETA-VAE beta 0.5 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "ptb BETA-VAE beta 0.1 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "ptb BETA-VAE beta 0 dec: Strong roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "ptb BETA-VAE beta 0 dec: Weak-M roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n",
      "ptb BETA-VAE beta 0.1 dec: Weak-M roBERTa\n",
      "torch.Size([2000, 62]) torch.Size([2000, 62])\n",
      "2000 2000\n"
     ]
    }
   ],
   "source": [
    "all_model_sample_lens = dict(ptb=dict(), yahoo_answer=dict())\n",
    "\n",
    "model_names = list(all_conditional_samples.keys())\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    \n",
    "    unconditional_samples = all_unconditional_samples[model_name][:N_UNCONDITIONAL_SAMPLES]\n",
    "    conditional_samples = all_conditional_samples[model_name]\n",
    "    \n",
    "    # to avoid lens > 62 only get the reasonable predictions\n",
    "    if \"weak\" in model_name.lower():\n",
    "        unconditional_samples = unconditional_samples[:, 1:-1]\n",
    "        conditional_samples = conditional_samples[:, 1:-1]\n",
    "    else:\n",
    "        unconditional_samples = unconditional_samples[:, :-1]\n",
    "        conditional_samples = conditional_samples[:, :-1]\n",
    "    \n",
    "    print(conditional_samples.shape, unconditional_samples.shape)\n",
    "    \n",
    "    unconditional_lens = get_length_of_chunk(unconditional_samples, remove_start=False)\n",
    "    conditional_lens = get_length_of_chunk(conditional_samples, remove_start=True)\n",
    "    \n",
    "    print(len(conditional_lens), len(unconditional_lens))\n",
    "    \n",
    "    if \"ptb\" in model_name.lower():\n",
    "        all_model_sample_lens[\"ptb\"][model_name] = dict(valid_conditional=conditional_lens, unconditional=unconditional_lens)\n",
    "    else:\n",
    "        all_model_sample_lens[\"yahoo_answer\"][model_name] = dict(valid_conditional=conditional_lens, unconditional=unconditional_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lens = {\n",
    "    \"ptb\": {\n",
    "        \"data_group\": dict(train=ptb_train_lens, valid=ptb_valid_lens),\n",
    "        \"model_groups\": all_model_sample_lens[\"ptb\"]},\n",
    "    \"yahoo_answer\": {\n",
    "        \"data_group\": dict(train=yahoo_train_lens, valid=yahoo_valid_lens),\n",
    "        \"model_groups\": all_model_sample_lens[\"yahoo_answer\"]}\n",
    "}\n",
    "torch.save(all_lens, \"length_analysis_data.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "thesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
